//
//  main.cpp
//  LoopClosure
//
//  Created by jimmy on 2016-02-16.
//  Copyright Â© 2016 jimmy. All rights reserved.
//

#include <iostream>
#include <fstream>
#include "cvxImage_310.hpp"
#include <string>
#include <vector>
#include "cvxIO.hpp"
#include <unordered_map>
#include "ms7ScenesUtil.hpp"
#include "bt_rnd_regressor_builder.h"
#include "bt_rnd_regressor.h"
#include "cvxImage_310.hpp"
#include "cvxIO.hpp"
#include "cvxPoseEstimation.hpp"
#include "ms7ScenesUtil.hpp"
#include "dataset_param.h"


using std::string;

using namespace::std;

#if 1

static void help()
{
    printf("program     modelFile  RGBImageList depthImageList cameraPoseList numSamplePerImage maxCheck predictions/prefix      \n");
    printf("BT_RND_test bt_RF.txt  rgbs.txt     depth.txt      poses.txt      5000              32       predictions/to3d  \n");
    printf("parameter fits to MS 7 Scenes, TUM dataset, 4 Scenes\n");
    printf("multiple 3D prediction ordered by feature distances, save files to result folder\n");
}

int main(int argc, const char * argv[])
{
    if (argc != 8) {
        printf("argc is %d, should be 8\n", argc);
        help();
        return -1;
    }
    
    const char * model_file = argv[1];
    const char * rgb_image_file = argv[2];
    const char * depth_image_file = argv[3];
    const char * camera_to_wld_pose_file = argv[4];
    const int num_random_sample = (int)strtod(argv[5], NULL);
    const int max_check = (int)strtod(argv[6], NULL);
   // const char * dataset_param_filename = argv[7];
    
    
    //const char *prediction_folder        = argv[2];
    const double inlierFeatDist = 0.3;
    const double inlierThreshold = 0.1;
    const double angleThreshold    = 5;
    const double distanceThreshold = 0.05;
    const char *prefix = argv[7];

    
    assert(num_random_sample > 100);
    // read test files (including camera pose files)
    vector<string> rgb_files   = Ms7ScenesUtil::read_file_names(rgb_image_file);
    vector<string> depth_files = Ms7ScenesUtil::read_file_names(depth_image_file);
    vector<string> pose_files  = Ms7ScenesUtil::read_file_names(camera_to_wld_pose_file);
    
    assert(rgb_files.size() == depth_files.size());
    assert(rgb_files.size() == pose_files.size());
    
    // read model
    BTRNDRegressor model;
    bool is_read = model.load(model_file);
    if (!is_read) {
        printf("Error: can not read from file %s\n", model_file);
        return -1;
    }
    
    const BTRNDTreeParameter & tree_param = model.getTreeParameter();
    const DatasetParameter  & dataset_param = model.getDatasetParameter();
    const bool use_depth = tree_param.is_use_depth_;
    if (use_depth) {
        printf("use depth in the feature.\n");
    }
    else {
        printf("not use depth in the feature.\n");
    }
    
    dataset_param.printSelf();
    tree_param.printSelf();
    
    cv::Mat camera_matrix = dataset_param.camera_matrix();
    const int wh_kernel_size = tree_param.wh_kernel_size_;
    const bool is_use_depth = tree_param.is_use_depth_;
    
    
    cv::Mat calibration_matrix = dataset_param.camera_matrix();
    const double depth_factor = dataset_param.depth_factor_;
    const double min_depth = dataset_param.min_depth_;
    const double max_depth = dataset_param.max_depth_;
    


    using FeatureType = SCRFRandomFeature;
    
    vector<double> angle_errors;
    vector<double> translation_errors;
    
    vector<cv::Mat> estimated_poses;
    ofstream error_file;
    error_file.open ("pose_error/error.txt");
    // read images, and predict one by one
    for (int k = 0; k<rgb_files.size(); k++)
    {
        clock_t begin1=clock();
        
        const char *cur_rgb_img_file     = rgb_files[k].c_str();
        const char *cur_depth_img_file   = depth_files[k].c_str();
        const char *cur_pose_file        = pose_files[k].c_str();
        
        cv::Mat rgb_img;
        CvxIO::imread_rgb_8u(cur_rgb_img_file, rgb_img);
        vector<FeatureType>     features;
        vector<Eigen::VectorXf> labels;
        BTRNDUtil::randomSampleFromRgbdImages(cur_rgb_img_file, cur_depth_img_file, cur_pose_file,
                                              num_random_sample, k, dataset_param,
                                              is_use_depth, false,
                                              features, labels);
        BTRNDUtil::extractWHFeatureFromRgbImages(cur_rgb_img_file, features, wh_kernel_size, false);
        assert(features.size() == labels.size());
        
        // predict from the model
        vector<vector<Eigen::VectorXf> > all_predictions;
        vector<vector<float> > all_distances; //color distance 
        vector<Eigen::VectorXf> all_labels;    // labels
        vector<Eigen::Vector2f> all_locations; // 2d location
        
        clock_t begin2 = clock();
        
        
        for(int j = 0; j<features.size(); j++)
        {
            vector<Eigen::VectorXf> preds;
            vector<float> dists;
            bool is_predict = model.predict(features[j], rgb_img, max_check, preds, dists);
            
            if(is_predict)
            {
                all_predictions.push_back(preds);
                all_distances.push_back(dists);
                all_labels.push_back(labels[j]);
                all_locations.push_back(features[j].p2d_);
            }
        }
        
        clock_t begin3 = clock();
        
        vector<cv::Point2d> img_pts; // img_pts for point on the matrix of graph 
        for(int m=0; m<all_locations.size(); m++)
        {
            double x_img= all_locations[m](0);
            double y_img= all_locations[m](1);
            img_pts.push_back(cv::Point2d(x_img,y_img));
        }
    
        vector<cv::Point3d> wld_pts_gt;// world coordinate based on camerapose  
        for(int m=0; m<all_labels.size(); m++)
        {
            double x_gt_world = all_labels[m](0);
            double y_gt_world = all_labels[m](1);
            double z_gt_world = all_labels[m](2);
            wld_pts_gt.push_back(cv::Point3d(x_gt_world, y_gt_world, z_gt_world));
        }
    
       vector<vector<cv::Point3d> > wld_pts_pred_candidate; // predicted pts 
        for(int m=0; m<all_predictions.size(); m++)
        {
            vector<cv::Point3d> tmp_wld_pred;
            for(int n=0; n<all_predictions[m].size(); n++)
            {
                double x_pred_world = all_predictions[m][n](0);
                double y_pred_world = all_predictions[m][n](1);
                double z_pred_world = all_predictions[m][n](2);
                tmp_wld_pred.push_back(cv::Point3d(x_pred_world, y_pred_world, z_pred_world));
            }
            wld_pts_pred_candidate.push_back(tmp_wld_pred);
        }
    
        string depth_img_file = depth_files[k];
        string camera_pose_file = pose_files[k];
        cv::Mat depth_img;
        CvxIO::imread_depth_16bit_to_64f(depth_img_file.c_str(), depth_img);
        
        cv::Mat camera_to_world_pose = Ms7ScenesUtil::read_pose_7_scenes(camera_pose_file.c_str());
        
        cv::Mat mask;
        cv::Mat camera_coordinate_position;
        cv::Mat wld_coord = Ms7ScenesUtil::cameraDepthToWorldCoordinate(depth_img,
                                                                        camera_to_world_pose,
                                                                        calibration_matrix,
                                                                        depth_factor,
                                                                        min_depth,
                                                                        max_depth,
                                                                        camera_coordinate_position,
                                                                        mask);
        
        // 2D location to 3D camera coordiante location* (location in camera to location in world coordinate)
        vector<vector<cv::Point3d> > valid_wld_pts_candidate;
        vector<cv::Point3d> valid_camera_pts; // ->world coordinates calculated by depth  
        for(int i = 0; i<img_pts.size(); i++) 
	{
            int x = img_pts[i].x;
            int y = img_pts[i].y;

            if(mask.at<unsigned char>(y, x) != 0) {
                cv::Point3d p = cv::Point3d(camera_coordinate_position.at<cv::Vec3d>(y, x));
                valid_camera_pts.push_back(p);
                valid_wld_pts_candidate.push_back(wld_pts_pred_candidate[i]);
            }
        }
        
        cv::Mat estimated_camera_pose = cv::Mat::eye(4, 4, CV_64F);
        if (valid_camera_pts.size() < 20) 
	{
            angle_errors.push_back(180.0);
            translation_errors.push_back(10.0);
            estimated_poses.push_back(estimated_camera_pose);
            continue;
        }
        
        // estimate camera pose using Kabsch
        PreemptiveRANSAC3DParameter param;
        param.dis_threshold_ = inlierThreshold;
        bool isEstimated = CvxPoseEstimation::preemptiveRANSAC3DOneToMany(valid_camera_pts, valid_wld_pts_candidate, param, estimated_camera_pose);
        if (isEstimated) {
            double angle_dis = 0.0;
            double location_dis = 0.0;
            cv::Mat gt_pose = Ms7ScenesUtil::read_pose_7_scenes(camera_pose_file.c_str());
            CvxPoseEstimation::poseDistance(gt_pose, estimated_camera_pose, angle_dis, location_dis);
            angle_errors.push_back(angle_dis);
            translation_errors.push_back(location_dis);
            printf("angle distance, location distance are %lf %lf\n", angle_dis, location_dis);
	    error_file << "angle distance " << angle_dis << " location distance " << location_dis << endl ;
        }
        else
        {
            angle_errors.push_back(180.0);
            translation_errors.push_back(10.0);
        }
        
        cout<<"estimated_camera_pose"<<estimated_camera_pose<<endl;
        clock_t end2 = clock();
        double feature_extraction_time = double(begin2-begin1)/(double)CLOCKS_PER_SEC;
        double forest_prediction_time = double(begin3-begin2)/(double)CLOCKS_PER_SEC;
        double test_estimate_time = double(end2 - begin2)/(double)CLOCKS_PER_SEC;
        cout.precision(5);
        cout<<"feature extraction time "<<feature_extraction_time<<endl;
        cout<<"forest prediction time "<<forest_prediction_time<<endl;
        cout<<"camera relocalization time "<<test_estimate_time<<endl;

        estimated_poses.push_back(estimated_camera_pose);
        
	for (int k = 0; k<estimated_poses.size(); k++) {
        char save_file[1024] = {NULL};
        sprintf(save_file, "%s_%06d.txt", prefix, k);
        FILE *pf = fopen(save_file, "w");
        assert(pf);
        //fprintf(pf, "%s\n", rgb_img_files[k].c_str());
        //fprintf(pf, "%s\n", depth_img_files[k].c_str());
        //fprintf(pf, "%s\n", camera_pose_files[k].c_str());
        Mat pose = estimated_poses[k];
        for (int r = 0; r<4; r++) {
            for (int c = 0; c<4; c++) {
                fprintf(pf, "%lf\t", pose.at<double>(r, c));
            }
            fprintf(pf, "\n");
        }
        fclose(pf);
    }
    printf("save to %s\n", prefix);
    
        
     
    }
  
    return 0;
}

#endif
